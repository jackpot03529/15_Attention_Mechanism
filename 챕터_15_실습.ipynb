{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 15-03 양방향 LSTM과 어텐션 메커니즘 (BiLSTM with Attention Mechanism)"
      ],
      "metadata": {
        "id": "jtr1Y5jG3bz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. IMDB 리뷰 데이터 전처리하기"
      ],
      "metadata": {
        "id": "RyLZILnK3pS4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "h6N5-NNM3rZh"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 10000\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = vocab_size)\n",
        "\n",
        "# 최대 단어 개수를 10,000으로 제한, 훈련 데이터와 테스트 데이터 받아온다.\n",
        "# 훈련 데이터와 레이블이 각각 X_train, y_train에 테스트 데이터 저장\n",
        "# 테스트 데이터와 이에 대한 레이블이 X_text, y_test에 저장\n",
        "# IMDB 리뷰 데이터는 이미 정수 인코딩이 되어있어, 남은 전처리는 패딩 뿐이다"
      ],
      "metadata": {
        "id": "mhIUDrAX3szc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('리뷰의 최대 길이 : {}'.format(max(len(l) for l in X_train)))\n",
        "print('리뷰의 평균 길이 : {}'.format(sum(map(len, X_train))/len(X_train)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iiXSuchm3-X4",
        "outputId": "6824d7d8-5500-4070-9c41-e4c29b191f54"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "리뷰의 최대 길이 : 2494\n",
            "리뷰의 평균 길이 : 238.71364\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 500\n",
        "X_train = pad_sequences(X_train, maxlen=max_len)\n",
        "X_test = pad_sequences(X_test, maxlen=max_len)\n",
        "\n",
        "# 길이 500으로 설정하고 패딩."
      ],
      "metadata": {
        "id": "SZkEcw-a4WV6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. 바다나우 어텐션(Bahdanau Attention)\n",
        "\n",
        "- 어텐션 함수란 주어진 query와 모든 key에 대해서 유사도를 측정하는 함수를 말한다."
      ],
      "metadata": {
        "id": "GmTtr7824yAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class BahdanauAttention(tf.keras.Model):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = Dense(units)\n",
        "    self.W2 = Dense(units)\n",
        "    self.V = Dense(1)\n",
        "\n",
        "  def call(self, values, query): # 단, key와 value는 같음\n",
        "    # query shape == (batch_size, hidden size)\n",
        "    # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # score 계산을 위해 뒤에서 할 덧셈을 위해서 차원을 변경해줍니다.\n",
        "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "metadata": {
        "id": "s87SCLGL4skh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. 양방향 LSTM + 어텐션 메커니즘(BiLSTM with Attention Mechanism)"
      ],
      "metadata": {
        "id": "c1QTHlV37gh7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dense, Embedding, Bidirectional, LSTM, Concatenate, Dropout\n",
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras import optimizers\n",
        "import os"
      ],
      "metadata": {
        "id": "6hG5Rta759Te"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 모델 설계 시작"
      ],
      "metadata": {
        "id": "vb7tKQFL7oll"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_input = Input(shape=(max_len,), dtype='int32')\n",
        "embedded_sequences = Embedding(vocab_size, 128, input_length=max_len, mask_zero = True)(sequence_input)\n",
        "# 입력층과 임베딩층을 설계\n",
        "# 10,000개의 단어들을 128차원의 벡터로 임베딩하도록 설계"
      ],
      "metadata": {
        "id": "ubTJU6_b7oAQ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm = Bidirectional(LSTM(64, dropout=0.5, return_sequences = True))(embedded_sequences)\n",
        "# 양방향 LSTM 설계.\n",
        "# 여기서는 두 층 사용\n",
        "# 우선 첫번째 층 만든다, 두번째 층을 위해 쌓을 예정이므로 return_sequences를 True로 해줘야 한다"
      ],
      "metadata": {
        "id": "SbW69ZYg7u_E"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 두번째 층 설계, 상태 리턴 받아야해서 return_state를 True로\n",
        "lstm, forward_h, forward_c, backward_h, backward_c = Bidirectional \\\n",
        "  (LSTM(64, dropout=0.5, return_sequences=True, return_state=True))(lstm)"
      ],
      "metadata": {
        "id": "qVap4KQH8Ris"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 각 상태의 크기(shape) 출력\n",
        "print(lstm.shape, forward_h.shape, forward_c.shape, backward_h.shape, backward_c.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbxIosJ_8ZOq",
        "outputId": "09f13fa4-c0db-4536-954b-6d46aab9bd5d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 500, 128) (None, 64) (None, 64) (None, 64) (None, 64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 순방향 LSTM의 은닉 상태와 셀상태를 forward_h, forward_c에 저장하고 역방향의 LSTM의 은닉 상태와 셀 상태를 backward_h, backward_c에 저장.\n",
        "- 은닉 상태나 셀 상태의 경우에는 128차원을 가진다, lstm의 경우는 500x128의 크기를 가진다.\n",
        "- forward 방향과 backward 방향이 연결된 hidden state 벡터가 모든 시점에 대해서 존재함을 의미한다.\n",
        "- 양방향 LSTM을 사용할 경우, 순방향 LSTM과 역방향 LSTM 각각 은닉 상태와 셀 상태를 가진다. 양방향 LSTM의 은닉 상태와 셀 상태를 사용하려면 두 방향의 LSTM의 상태들을 연결(concatenate)해주면 된다."
      ],
      "metadata": {
        "id": "cUABp9Jn9ZbL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "state_h = Concatenate()([forward_h, backward_h]) # 은닉 상태\n",
        "state_c = Concatenate()([forward_c, backward_c]) # 셀 상태"
      ],
      "metadata": {
        "id": "xgVWGLBG8eji"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 어텐션 메커니즘에서는 은닉상태 사용. 이를 입력으로 컨텍스트 벡터(context vector) 얻는다\n",
        "attention = BahdanauAttention(64) # 가중치 크기 정의\n",
        "context_vector, attention_weights = attention(lstm, state_h)"
      ],
      "metadata": {
        "id": "cUitws8Y-wgg"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 컨텍스트 벡터를 밀집층(dense layer)에 통과 시키고, 이진 분류이므로 최대 출력층에 1개의 뉴런을 배치, 활성화 함수로 시그모이드 함수 사용"
      ],
      "metadata": {
        "id": "cf83VBc5_DqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dense1 = Dense(20, activation=\"relu\")(context_vector)\n",
        "dropout = Dropout(0.5)(dense1)\n",
        "output = Dense(1, activation=\"sigmoid\")(dropout)\n",
        "model = Model(inputs=sequence_input, outputs=output)"
      ],
      "metadata": {
        "id": "sUzspqUd_CLX"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 옵티마이저로 아담 옵티마이저 사용, 모델 컴파일\n",
        "# 시그모이드 함수를 사용하므로, 손실 함수로 binary_crossentropy 사용\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "psCAqsE3_SXB"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 훈련\n",
        "history = model.fit(X_train, y_train, epochs = 3, batch_size = 256, validation_data=(X_test, y_test), verbose=1)\n",
        "# 검증 데이터로 테스트 데이터 사용해서 에포크가 끝날 때마다 테스트 데이터에 대한 정확도 출력"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmlPSjaS_gVh",
        "outputId": "60402c97-cec1-4498-c1ce-825be82c8e3c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "98/98 [==============================] - 1326s 13s/step - loss: 0.4959 - accuracy: 0.7610 - val_loss: 0.3157 - val_accuracy: 0.8628\n",
            "Epoch 2/3\n",
            "98/98 [==============================] - 1293s 13s/step - loss: 0.2584 - accuracy: 0.9070 - val_loss: 0.2934 - val_accuracy: 0.8800\n",
            "Epoch 3/3\n",
            "98/98 [==============================] - 1289s 13s/step - loss: 0.1988 - accuracy: 0.9312 - val_loss: 0.2947 - val_accuracy: 0.8773\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n 테스트 정확도: %.4f\" % (model.evaluate(X_test, y_test)[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YREiHfWr_ilJ",
        "outputId": "084acf18-7dbb-4db1-c8e7-744dd6fc6c71"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 343s 439ms/step - loss: 0.2947 - accuracy: 0.8773\n",
            "\n",
            " 테스트 정확도: 0.8773\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P0O7N5-b_uxK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}